---
layout: post
title:  招标网爬虫工具
date:   2023-07-27 10:11:50 +0300
img: 8.jpg
tags: [python, spider]
---

# 1 概述
## 1.1 爬取需求
爬取招标网 https://www.zhaobiao.cn/ 中标公告、招标公告等详细信息（如：品牌、供应商、招标金额、中标金额等）。

![]({{site.baseurl}}/images/15.jpg)

## 1.2 使用工具和环境
### 1.2.1 使用工具
pycharm
### 1.2.2 环境
python 3.9

# 2 爬虫工具设计说明
## 2.1 所需要的库
{% highlight markdown %}
from selenium import webdriver
from selenium.webdriver.common.by import By
from PIL import Image
import ddddocr
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
import time
import streamlit as st
import base64
import requests
from lxml import etree
import re
import urllib.parse
from io import BytesIO
import pandas as pd
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
{% endhighlight %}
**简介：**这段代码是导入所需的库和模块，以及相关的函数和类。这些库和模块用于实现数据爬取、网页操作、验证码识别、数据处理、Excel操作和Web应用程序开发。

**具体导入的库和模块如下：**
selenium: 用于进行网页操作和自动化测试。
PIL: Python Imaging Library，用于图像处理和操作。
ddddocr: 用于验证码识别。
streamlit: 用于构建Web应用程序的交互式界面。
base64: 用于处理Base64编码的数据。
requests: 用于发送HTTP请求和处理响应。
lxml: 用于解析和处理HTML/XML数据。
re: 用于正则表达式操作。
urllib.parse: 用于URL编解码。
io.BytesIO: 用于创建内存文件对象。
pandas: 用于数据处理和操作。
selenium.webdriver.chrome.options.Options: 用于配置Chrome浏览器的选项。
selenium.webdriver.chrome.service.Service: 用于配置Chrome浏览器的服务。
webdriver_manager.chrome.ChromeDriverManager: 用于管理和下载Chrome浏览器驱动。

## 2.2 各个函数模块描述
### 2.2.1 自动获取网页cookie
{% highlight markdown %}
def Get_Cookies(url_login, url_target, user_name, secret):
    strr = ''  # 创建空的cookie值
    with st.spinner('Loading cookie...'):
        while (True):
            options = Options()
            options.add_argument('--disable-gpu')
            options.add_argument('--headless')
            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
            driver.get(url_login)

            zhanghao_mima = driver.find_element(By.ID, 'zh')
            zhanghao_mima.click()
            # 定位账号，密码，验证码和登录按钮部分
            username_input = driver.find_element(By.ID, 'loginUserId')
            password_input = driver.find_element(By.ID, 'loginPassword')
            captcha = driver.find_element(By.ID, 'yzm')
            login_button = driver.find_element(By.CLASS_NAME, 'login_button')
            username_input.send_keys(user_name)  # 填写用户名 
            password_input.send_keys(secret)  # 填写密码 
            # 识别验证码部分
            png = driver.find_element(By.ID, 'randimg')
            screenshot = png.screenshot_as_png  # 获取屏幕截图的二进制数据
            image_stream = BytesIO(screenshot)  # 使用BytesIO创建一个内存文件对象
            image = Image.open(image_stream)  # 通过内存文件对象加载图像
            ocr = ddddocr.DdddOcr()  # 验证码识别库
            res = ocr.classification(image)
            captcha.send_keys(res)  # 输入识别的验证码
            login_button.click()
            # 等待页面跳转
            wait = WebDriverWait(driver, 10)
            try:
                wait.until(EC.url_to_be(url_target))
                time.sleep(5)
                cookie = driver.get_cookies()
                strr = ''
                for c in cookie:
                    strr += c['name']
                    strr += '='
                    strr += c['value']
                    strr += ';'
                st.success('获取cookies成功！')
                image_stream.close()  # 关闭内存文件对象
                break
            except:
                image_stream.close()  # 关闭内存文件对象
                continue
    return strr
{% endhighlight %}
**简介：**用Selenium库来自动化Chrome浏览器，并利用第三方验证码识别库（ddddocr）来处理验证码。

**详细步骤：**
1．	创建一个空字符串strr，用于存储Cookie值。
2．	使用Selenium的webdriver.Chrome创建一个无头浏览器实例，以便在后台执行操作（不可见）。
3．	打开登录URL，并定位用户名、密码、验证码和登录按钮的元素。
4．	输入提供的用户名和密码。
5．	获取验证码图片并进行验证码识别。它使用ddddocr库来对验证码图片进行识别，并将识别结果输入到验证码输入框。
6．	点击登录按钮进行登录，并等待页面跳转到指定的目标URL（url_target）。
7．	一旦页面跳转到目标URL，获取浏览器的Cookie值，并将其拼接为一个字符串strr。
8．	在成功获取Cookie后，关闭验证码图片的内存文件对象，完成函数操作。


### 2.2.2 创建DataFrame和转换Excel
#### 2.2.2.1 创建DataFrame
{% highlight markdown %}
def Create_dataframe():
    # 为创建的excel表格添加表头
    data_list = ['类型', '标题', '地区', '发布时间', '品牌', '供应商', '中标金额', '招标金额', '标的物', '对应网址',
                 '附件']
    # 创建空的DataFrame
    df = pd.DataFrame(columns=data_list)
    return df
{% endhighlight %}
**简介：**创建一个空的DataFrame，用于存储数据，并设置了一些列的表头，以便稍后将数据填充到DataFrame中。

#### 2.2.2.2 DataFrame转换Excel
{% highlight markdown %}
def download_df_to_excel():
    # 创建一个内存文件对象
    excel_buffer = BytesIO()
    # 将 DataFrame 保存到内存文件对象中
    with pd.ExcelWriter(excel_buffer, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False, sheet_name='Sheet1')
    # 获取内存文件对象的二进制数据
    excel_data = excel_buffer.getvalue()
    # 提供下载链接
    b64 = base64.b64encode(excel_data).decode()
    href = f'<a href="data:application/octet-stream;base64,{b64}" download="data.xlsx">Download Excel File</a>'
    st.markdown(href, unsafe_allow_html=True)
{% endhighlight %}
**简介：**这段代码用于将数据保存为Excel文件并提供下载链接。代码中使用了Pandas和XlsxWriter库来实现这个功能。

**详细步骤：**
1.	创建一个内存文件对象 excel_buffer，这里使用了BytesIO，将数据保存在内存中而不是硬盘上。
2.	使用pd.ExcelWriter创建一个Excel写入器（writer），将数据写入Excel文件。指定了engine='xlsxwriter'，这是Pandas支持的一种Excel写入引擎。
3.	使用df.to_excel()方法将DataFrame df 写入Excel文件中。index=False表示不包含索引列，sheet_name='Sheet1'表示将数据写入名为'Sheet1'的工作表。
4.	将DataFrame数据保存到内存文件对象中。
5.	获取内存文件对象的二进制数据excel_data。
6.	使用base64库将二进制数据编码为base64格式，并将其作为下载链接的一部分。
7.	创建一个包含下载链接的HTML元素，并使用st.markdown在Streamlit应用中显示链接。用户可以点击这个链接来下载Excel文件。

### 2.2.3 分析主网页并获取详情页urls
#### 2.2.3.1 爬取一年内数据主网页分析
{% highlight markdown %}
def Analyze_Main_Url_1(main_url, keyword, Information_category, start_time, end_time, i, cookie):
    headers_1 = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Cache-Control': 'max-age=0',
        'Connection': 'keep-alive',
        'Content-Length': '233',
        'Content-Type': 'application/x-www-form-urlencoded',
        'Cookie': cookie,
        'Host': 's.zhaobiao.cn',
        'Origin': 'https://s.zhaobiao.cn',
        'Referer': 'https://s.zhaobiao.cn/s',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.170 Safari/537.36'
    }
    from_data_1 = {
        'queryword': keyword,
        'channels': Information_category,
        'starttime': start_time,
        'endtime': end_time,
        'currentpage': i
    }
    response_1 = requests.post(main_url, headers=headers_1, data=from_data_1)
    html_1 = etree.HTML(response_1.text)
    detail_Urls = html_1.xpath('.//tr[@class = "datatr"]')
    return detail_Urls
{% endhighlight %}
**简介：**这段代码用于分析主URL（main_url）上的网页内容，并根据提供的关键字（keyword）、信息类别（Information_category）、起始时间（start_time）、结束时间（end_time）、当前页数（i）和Cookie信息（cookie）来进行搜索请求。

**详细步骤：**
1.	定义了HTTP请求头（headers_1），其中包含了一些常见的请求头字段，如User-Agent、Cookie等。这些请求头信息用于模拟一个普通的浏览器请求，以便能够访问网站并获取数据。
2.	定义了表单数据（from_data_1），包含了搜索请求的参数，如关键字、信息类别、起始时间、结束时间和当前页数。
3.	使用requests.post方法发送POST请求到main_url，并传递请求头和表单数据。这个请求将触发对main_url的搜索操作，并返回一个包含搜索结果的网页响应。
4.	使用etree.HTML方法解析网页响应的文本内容，将其转换成一个XPath可解析的格式。
5.	使用XPath表达式.//tr[@class = "datatr"]从解析后的HTML中提取具有class属性为datatr的tr标签元素。这个XPath表达式用于从搜索结果页面中提取各个条目的URL链接（详情页链接）。
6.	返回提取到的详情页链接列表 detail_Urls。

#### 2.2.3.2 爬取历史数据库主网页分析
{% highlight markdown %}
def Analyze_Main_Url_2(main_url, keyword, Information_category, year, month, i, cookie):
    headers_1 = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Cache-Control': 'max-age=0',
        'Connection': 'keep-alive',
        'Content-Length': '233',
        'Content-Type': 'application/x-www-form-urlencoded',
        'Cookie': cookie,
        'Host': 's.zhaobiao.cn',
        'Origin': 'https://s.zhaobiao.cn',
        'Referer': 'https://s.zhaobiao.cn/s',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.170 Safari/537.36'
    }
    from_data_1 = {
        'hisType': 'zb',
        'queryword': keyword,
        'channels': Information_category,
        'year': year,
        'month': month,
        'currentpage': i
    }
    response_2 = requests.post(main_url, headers=headers_1, data=from_data_1)
    html_2 = etree.HTML(response_2.text)
    detail_Urls = html_2.xpath('.//tr[@style = "border-bottom:1px dashed #ccc;"]')
    return detail_Urls
{% endhighlight %}
**简介：**这段代码是用于分析主URL（main_url）上的网页内容，并根据提供的关键字（keyword）、信息类别（Information_category）、年份（year）、月份（month）、当前页数（i）和Cookie信息（cookie）进行搜索请求。

**详细步骤：**
1.	定义了HTTP请求头（headers_1），其中包含了一些常见的请求头字段，如User-Agent、Cookie等。这些请求头信息用于模拟一个普通的浏览器请求，以便能够访问网站并获取数据。
2.	定义了表单数据（from_data_1），包含了搜索请求的参数，如关键字、信息类别、年份、月份和当前页数。
3.	使用requests.post方法发送POST请求到main_url，并传递请求头和表单数据。这个请求将触发对main_url的搜索操作，并返回一个包含搜索结果的网页响应。
4.	使用etree.HTML方法解析网页响应的文本内容，将其转换成一个XPath可解析的格式。
5.	使用XPath表达式.//tr[@style = "border-bottom:1px dashed #ccc;"]从解析后的HTML中提取具有指定样式的tr标签元素。这个XPath表达式用于从搜索结果页面中提取各个条目的URL链接（详情页链接）。
6.	返回提取到的详情页链接列表 detail_Urls。

### 2.2.4 筛选网页
{% highlight markdown %}
def Sift_Url(detail_Url, index, keyword_list, cookie):
    headers_2 = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Cache-Control': 'max-age=0',
        'Connection': 'keep-alive',
        'Content-Length': '46',
        'Content-Type': 'application/x-www-form-urlencoded',
        'Cookie': cookie,
        'Host': 'zb.zhaobiao.cn',
        'Origin': 'https://s.zhaobiao.cn',
        'Referer': 'https://s.zhaobiao.cn/s',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.170 Safari/537.36'
    }
    from_data_2 = {
        'q': '%D0%C4%B5%E7%CD%F8%C2%E7',
        'm': '1'
    }
    url = detail_Url.xpath('td/a/@href')[index]
    st.write("正在爬取的网页是:", url)
    response_2 = session.post(url, headers=headers_2, data=from_data_2)
    html_2 = etree.HTML(response_2.text)
    contents = html_2.xpath('.//div[@class = "w-noticeCont"]/div[@class = "w-contIn page"]/script/text()')[0].strip()
    # 网页信息被编码了，提取编码
    secret = re.findall('var ss = "(.*)";', contents)[0]
    # 解码
    pass_word = urllib.parse.unquote(urllib.parse.unquote(secret))
    html_3 = etree.HTML(pass_word)
    texts = html_3.xpath('//text()')
    combine_texts = ' '.join(texts)
    for th in keyword_list:
        if th in combine_texts:
            type = detail_Url.xpath('td[@align="center"]/text()')[0]  # 类型
            title = html_2.xpath('//h1[@id="infotitle"]/text()')[0]  # 标题
            place = detail_Url.xpath('td[@align="center"]/text()')[1]  # 地点
            release_time = detail_Url.xpath('td[@align="center"]/text()')[2]  # 时间
            return type, title, place, release_time, html_3, url, th
{% endhighlight %}
**简介：**这段代码用于从给定的详情页URL（detail_Url）中筛选和提取相关信息。它使用提供的cookie信息以及其他请求头和表单数据进行请求，然后解析网页内容并提取指定信息。

**详细步骤：**
1.	定义了HTTP请求头（headers_2），其中包含了一些常见的请求头字段，如User-Agent、Cookie等。这些请求头信息用于模拟一个普通的浏览器请求，以便能够访问网页并获取数据。
2.	定义了表单数据（from_data_2），其中包含了请求参数。
3.	从detail_Url中提取指定索引（index）位置的URL，并使用session.post方法发送POST请求，以获取详情页的内容。
4.	解析网页内容并提取编码的信息（secret）。
5.	对编码的信息进行解码，得到详情页的HTML内容（pass_word）。
6.	从HTML内容中提取文本，并将其组合成一个字符串 combine_texts。
7.	遍历关键词列表 keyword_list，检查每个关键词是否出现在提取的文本中。如果关键词出现在文本中，那么将提取相关信息并返回。
8.	返回提取到的类型、标题、地点、发布时间、详情页的HTML内容、详情页的URL和匹配的关键词。
