---
layout: post
title: 招标网爬虫工具
date: 2023-07-27 10:11:50 +0300
img: 21.jpg
tags: [Python, Spider]
author: Li Haopeng
---

# 1 概述
## 1.1 爬取需求
爬取招标网 https://www.zhaobiao.cn/ 中标公告、招标公告等详细信息（如：品牌、供应商、招标金额、中标金额等）。

![]({{site.baseurl}}/images/zhaobiao_spider/1.jpg)

## 1.2 使用工具和环境
### 1.2.1 使用工具
pycharm
### 1.2.2 环境
python 3.9

## 1.3 爬虫设计思路
__构建交互页面__&rarr;__获取登录网页cookie__&rarr;__获取爬取目标网页总页数__&rarr;__分析主网页并筛选网页__&rarr;__获取目标数据并保存excel__

## 1.4 网页分析中可能存在的疑惑
**如何实现翻页、时间范围等限制条件：**
1. 在主网页输入关键词，点击你想要获取的内容（如：招标公告、中标公告等），点击定义时间范围，在完成所需要的设定后，打开开发者面板。
![页面展示]({{site.baseurl}}/images/zhaobiao_spider/8.jpg)
2. 找到对应的页面
![]({{site.baseurl}}/images/zhaobiao_spider/9.jpg)
3. 所需要的数据在表单里面
* 可以看到在这个表单数据里面有着定义时间、当前页号、总页数等
![]({{site.baseurl}}/images/zhaobiao_spider/10.jpg)
4. 通过在request里面添加请求体就可以实现想要的条件

# 2 爬虫工具设计说明
## 2.1 所需要的库
```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from PIL import Image
import ddddocr
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
import time
import streamlit as st
import base64
import requests
from lxml import etree
import re
import urllib.parse
from io import BytesIO
import pandas as pd
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
```

**简介：**
这段代码是导入所需的库和模块，以及相关的函数和类。这些库和模块用于实现数据爬取、网页操作、验证码识别、数据处理、Excel操作和Web应用程序开发。

**具体导入的库和模块如下：**
* selenium: 用于进行网页操作和自动化测试。
* PIL: Python Imaging Library，用于图像处理和操作。
* ddddocr: 用于验证码识别。
* streamlit: 用于构建Web应用程序的交互式界面。
* base64: 用于处理Base64编码的数据。
* requests: 用于发送HTTP请求和处理响应。
* lxml: 用于解析和处理HTML/XML数据。
* re: 用于正则表达式操作。
* urllib.parse: 用于URL编解码。
* io.BytesIO: 用于创建内存文件对象。
* pandas: 用于数据处理和操作。
* selenium.webdriver.chrome.options.Options: 用于配置Chrome浏览器的选项。
* selenium.webdriver.chrome.service.Service: 用于配置Chrome浏览器的服务。
* webdriver_manager.chrome.ChromeDriverManager: 用于管理和下载Chrome浏览器驱动。

## 2.2 各个函数模块描述
### 2.2.1 自动获取网页cookie
数据的获取需要通过账号密码的登录页面数据才能显示，在爬取时需要获取到登录后的cookie。
```python
def Get_Cookies(url_login, url_target, user_name, secret):
    strr = ''  # 创建空的cookie值
    with st.spinner('Loading cookie...'):
        while (True):
            options = Options()
            options.add_argument('--disable-gpu')
            options.add_argument('--headless')
            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
            driver.get(url_login)

            zhanghao_mima = driver.find_element(By.ID, 'zh')
            zhanghao_mima.click()
            # 定位账号，密码，验证码和登录按钮部分
            username_input = driver.find_element(By.ID, 'loginUserId')
            password_input = driver.find_element(By.ID, 'loginPassword')
            captcha = driver.find_element(By.ID, 'yzm')
            login_button = driver.find_element(By.CLASS_NAME, 'login_button')
            username_input.send_keys(user_name)  # 填写用户名 
            password_input.send_keys(secret)  # 填写密码 
            # 识别验证码部分
            png = driver.find_element(By.ID, 'randimg')
            screenshot = png.screenshot_as_png  # 获取屏幕截图的二进制数据
            image_stream = BytesIO(screenshot)  # 使用BytesIO创建一个内存文件对象
            image = Image.open(image_stream)  # 通过内存文件对象加载图像
            ocr = ddddocr.DdddOcr()  # 验证码识别库
            res = ocr.classification(image)
            captcha.send_keys(res)  # 输入识别的验证码
            login_button.click()
            # 等待页面跳转
            wait = WebDriverWait(driver, 10)
            try:
                wait.until(EC.url_to_be(url_target))
                time.sleep(5)
                cookie = driver.get_cookies()
                strr = ''
                for c in cookie:
                    strr += c['name']
                    strr += '='
                    strr += c['value']
                    strr += ';'
                st.success('获取cookies成功！')
                image_stream.close()  # 关闭内存文件对象
                break
            except:
                image_stream.close()  # 关闭内存文件对象
                continue
    return strr
```

**简介：**
用Selenium库来自动化Chrome浏览器，并利用第三方验证码识别库（ddddocr）来处理验证码。

**详细步骤：**
1.	创建一个空字符串strr，用于存储Cookie值。
2.  使用Selenium的webdriver.Chrome创建一个无头浏览器实例，以便在后台执行操作（不可见）。
3. 	打开登录URL，并定位用户名、密码、验证码和登录按钮的元素。
4.	输入提供的用户名和密码。
5. 	获取验证码图片并进行验证码识别。它使用ddddocr库来对验证码图片进行识别，并将识别结果输入到验证码输入框。
6. 	点击登录按钮进行登录，并等待页面跳转到指定的目标URL（url_target）。
7.	一旦页面跳转到目标URL，获取浏览器的Cookie值，并将其拼接为一个字符串strr。
8.	在成功获取Cookie后，关闭验证码图片的内存文件对象，完成函数操作。


### 2.2.2 创建DataFrame和转换Excel
#### 2.2.2.1 创建DataFrame
```python
def Create_dataframe():
    # 为创建的excel表格添加表头
    data_list = ['类型', '标题', '地区', '发布时间', '品牌', '供应商', '中标金额', '招标金额', '标的物', '对应网址',
                 '附件']
    # 创建空的DataFrame
    df = pd.DataFrame(columns=data_list)
    return df
```
**简介：**
创建一个空的DataFrame，用于存储数据，并设置了一些列的表头，以便稍后将数据填充到DataFrame中。

#### 2.2.2.2 DataFrame转换Excel
```python
def download_df_to_excel():
    # 创建一个内存文件对象
    excel_buffer = BytesIO()
    # 将 DataFrame 保存到内存文件对象中
    with pd.ExcelWriter(excel_buffer, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False, sheet_name='Sheet1')
    # 获取内存文件对象的二进制数据
    excel_data = excel_buffer.getvalue()
    # 提供下载链接
    b64 = base64.b64encode(excel_data).decode()
    href = f'<a href="data:application/octet-stream;base64,{b64}" download="data.xlsx">Download Excel File</a>'
    st.markdown(href, unsafe_allow_html=True)
```
**简介：**
这段代码用于将数据保存为Excel文件并提供下载链接。代码中使用了Pandas和XlsxWriter库来实现这个功能。

**详细步骤：**
1.	创建一个内存文件对象 excel_buffer，这里使用了BytesIO，将数据保存在内存中而不是硬盘上。
2.	使用pd.ExcelWriter创建一个Excel写入器（writer），将数据写入Excel文件。指定了engine='xlsxwriter'，这是Pandas支持的一种Excel写入引擎。
3.	使用df.to_excel()方法将DataFrame df 写入Excel文件中。index=False表示不包含索引列，sheet_name='Sheet1'表示将数据写入名为'Sheet1'的工作表。
4.	将DataFrame数据保存到内存文件对象中。
5.	获取内存文件对象的二进制数据excel_data。
6.	使用base64库将二进制数据编码为base64格式，并将其作为下载链接的一部分。
7.	创建一个包含下载链接的HTML元素，并使用st.markdown在Streamlit应用中显示链接。用户可以点击这个链接来下载Excel文件。

### 2.2.3 分析主网页并获取详情页urls
#### 2.2.3.1 爬取一年内数据主网页分析
```python
def Analyze_Main_Url_1(main_url, keyword, Information_category, start_time, end_time, i, cookie):
    headers_1 = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Cache-Control': 'max-age=0',
        'Connection': 'keep-alive',
        'Content-Length': '233',
        'Content-Type': 'application/x-www-form-urlencoded',
        'Cookie': cookie,
        'Host': 's.zhaobiao.cn',
        'Origin': 'https://s.zhaobiao.cn',
        'Referer': 'https://s.zhaobiao.cn/s',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.170 Safari/537.36'
    }
    from_data_1 = {
        'queryword': keyword,
        'channels': Information_category,
        'starttime': start_time,
        'endtime': end_time,
        'currentpage': i
    }
    response_1 = requests.post(main_url, headers=headers_1, data=from_data_1)
    html_1 = etree.HTML(response_1.text)
    detail_Urls = html_1.xpath('.//tr[@class = "datatr"]')
    return detail_Urls
```
**简介：**
这段代码用于分析主URL（main_url）上的网页内容，并根据提供的关键字（keyword）、信息类别（Information_category）、起始时间（start_time）、结束时间（end_time）、当前页数（i）和Cookie信息（cookie）来进行搜索请求。

**详细步骤：**
1.	定义了HTTP请求头（headers_1），其中包含了一些常见的请求头字段，如User-Agent、Cookie等。这些请求头信息用于模拟一个普通的浏览器请求，以便能够访问网站并获取数据。
2.	定义了表单数据（from_data_1），包含了搜索请求的参数，如关键字、信息类别、起始时间、结束时间和当前页数。
3.	使用requests.post方法发送POST请求到main_url，并传递请求头和表单数据。这个请求将触发对main_url的搜索操作，并返回一个包含搜索结果的网页响应。
4.	使用etree.HTML方法解析网页响应的文本内容，将其转换成一个XPath可解析的格式。
5.	使用XPath表达式.//tr[@class = "datatr"]从解析后的HTML中提取具有class属性为datatr的tr标签元素。这个XPath表达式用于从搜索结果页面中提取各个条目的URL链接（详情页链接）。
6.	返回提取到的详情页链接列表 detail_Urls。

#### 2.2.3.2 爬取历史数据库主网页分析
```python
def Analyze_Main_Url_2(main_url, keyword, Information_category, year, month, i, cookie):
    headers_1 = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Cache-Control': 'max-age=0',
        'Connection': 'keep-alive',
        'Content-Length': '233',
        'Content-Type': 'application/x-www-form-urlencoded',
        'Cookie': cookie,
        'Host': 's.zhaobiao.cn',
        'Origin': 'https://s.zhaobiao.cn',
        'Referer': 'https://s.zhaobiao.cn/s',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.170 Safari/537.36'
    }
    from_data_1 = {
        'hisType': 'zb',
        'queryword': keyword,
        'channels': Information_category,
        'year': year,
        'month': month,
        'currentpage': i
    }
    response_2 = requests.post(main_url, headers=headers_1, data=from_data_1)
    html_2 = etree.HTML(response_2.text)
    detail_Urls = html_2.xpath('.//tr[@style = "border-bottom:1px dashed #ccc;"]')
    return detail_Urls
```
**简介：**
这段代码是用于分析主URL（main_url）上的网页内容，并根据提供的关键字（keyword）、信息类别（Information_category）、年份（year）、月份（month）、当前页数（i）和Cookie信息（cookie）进行搜索请求。

**详细步骤：**
1.	定义了HTTP请求头（headers_1），其中包含了一些常见的请求头字段，如User-Agent、Cookie等。这些请求头信息用于模拟一个普通的浏览器请求，以便能够访问网站并获取数据。
2.	定义了表单数据（from_data_1），包含了搜索请求的参数，如关键字、信息类别、年份、月份和当前页数。
3.	使用requests.post方法发送POST请求到main_url，并传递请求头和表单数据。这个请求将触发对main_url的搜索操作，并返回一个包含搜索结果的网页响应。
4.	使用etree.HTML方法解析网页响应的文本内容，将其转换成一个XPath可解析的格式。
5.	使用XPath表达式.//tr[@style = "border-bottom:1px dashed #ccc;"]从解析后的HTML中提取具有指定样式的tr标签元素。这个XPath表达式用于从搜索结果页面中提取各个条目的URL链接（详情页链接）。
6.	返回提取到的详情页链接列表 detail_Urls。

### 2.2.4 筛选网页
```python
def Sift_Url(detail_Url, index, keyword_list, cookie):
    headers_2 = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Cache-Control': 'max-age=0',
        'Connection': 'keep-alive',
        'Content-Length': '46',
        'Content-Type': 'application/x-www-form-urlencoded',
        'Cookie': cookie,
        'Host': 'zb.zhaobiao.cn',
        'Origin': 'https://s.zhaobiao.cn',
        'Referer': 'https://s.zhaobiao.cn/s',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.170 Safari/537.36'
    }
    from_data_2 = {
        'q': '%D0%C4%B5%E7%CD%F8%C2%E7',
        'm': '1'
    }
    url = detail_Url.xpath('td/a/@href')[index]
    st.write("正在爬取的网页是:", url)
    response_2 = session.post(url, headers=headers_2, data=from_data_2)
    html_2 = etree.HTML(response_2.text)
    contents = html_2.xpath('.//div[@class = "w-noticeCont"]/div[@class = "w-contIn page"]/script/text()')[0].strip()
    # 网页信息被编码了，提取编码
    secret = re.findall('var ss = "(.*)";', contents)[0]
    # 解码
    pass_word = urllib.parse.unquote(urllib.parse.unquote(secret))
    html_3 = etree.HTML(pass_word)
    texts = html_3.xpath('//text()')
    combine_texts = ' '.join(texts)
    for th in keyword_list:
        if th in combine_texts:
            type = detail_Url.xpath('td[@align="center"]/text()')[0]  # 类型
            title = html_2.xpath('//h1[@id="infotitle"]/text()')[0]  # 标题
            place = detail_Url.xpath('td[@align="center"]/text()')[1]  # 地点
            release_time = detail_Url.xpath('td[@align="center"]/text()')[2]  # 时间
            return type, title, place, release_time, html_3, url, th
```
**简介：**
这段代码用于从给定的详情页URL（detail_Url）中筛选和提取相关信息。它使用提供的cookie信息以及其他请求头和表单数据进行请求，然后解析网页内容并提取指定信息。

**详细步骤：**
1.	定义了HTTP请求头（headers_2），其中包含了一些常见的请求头字段，如User-Agent、Cookie等。这些请求头信息用于模拟一个普通的浏览器请求，以便能够访问网页并获取数据。
2.	定义了表单数据（from_data_2），其中包含了请求参数。
3.	从detail_Url中提取指定索引（index）位置的URL，并使用session.post方法发送POST请求，以获取详情页的内容。
4.	解析网页内容并提取编码的信息（secret）。
5.	对编码的信息进行解码，得到详情页的HTML内容（pass_word）。
6.	从HTML内容中提取文本，并将其组合成一个字符串 combine_texts。
7.	遍历关键词列表 keyword_list，检查每个关键词是否出现在提取的文本中。如果关键词出现在文本中，那么将提取相关信息并返回。
8.	返回提取到的类型、标题、地点、发布时间、详情页的HTML内容、详情页的URL和匹配的关键词。

### 2.2.5 分析筛选出的网页并提取数据
```python
def analysis_detail_Url(from_data_1_cannels, html_3, thing):
    brand = ''
    supplier = ''
    succeed_price = ''
    bidding_price = ''
    # 构建网页中的表格信息
    table_elements = html_3.xpath('//table')
    for table_element in table_elements:  # 遍历网页中所有的表格（构造表格为竖状表格）
        table = []
        # 确认解码网页中是否含有thead标签
        thead_elements = table_element.xpath('.//thead/th')
        if thead_elements:
            for thead in thead_elements:
                table.append(thead.xpath('.//th/text()'))
        for trs in table_element.xpath('.//tr'):
            table_2 = []
            # 确认解码网页中是否含有th标签
            if len(trs.xpath('.//th')) > 1:
                table.append(trs.xpath('.//th/text()'))
            else:
                for td in trs.xpath('.//td'):
                    if td.xpath('.//text()'):
                        if td.xpath('.//p'):
                            table_2.append(td.xpath('.//p')[0].xpath('string()').replace('\n', '').replace(' ', ''))
                        elif td.xpath('.//div'):
                            table_2.append(td.xpath('.//div')[0].xpath('string()').replace('\n', '').replace(' ', ''))
                        else:
                            table_2.append(td.xpath('string()').replace('\n', '').replace(' ', ''))
                    else:
                        table_2.append('-')
            # 如果table_2不空，添加到table表格
            if len(table_2) > 0:
                table.append(table_2)
        if len(table) != 0:
            # 获取每行的最大列数
            max_cols = max(len(row) for row in table)
            # 输出对齐后的二维列表
            aligned_list = [row + ['-'] * (max_cols - len(row)) for row in table]
            f = 0  # 扫描列的初始索引
            if len(aligned_list) > 1:
                for index, row in enumerate(aligned_list):  # 输出关键词所在的行索引
                    row = ' '.join(row)
                    if thing in row or len(aligned_list) <= 2:
                        if thing in row:
                            k = index
                        else:
                            k = 1
                        for columns in aligned_list[0]:
                            if '品牌' in columns and brand == '':
                                brand = aligned_list[k][f]
                            if any(th in columns for th in
                                       succeed_prices) and succeed_price == '' and from_data_1_cannels == 'succeed':
                                succeed_price = aligned_list[k][f]
                            if any(th in columns for th in bidding_prices) and bidding_price == '' and (
                                        from_data_1_cannels == 'bidding' or from_data_1_cannels == 'cgall'):
                                bidding_price = aligned_list[k][f]
                            if any(th in columns for th in suppliers) and supplier == '':
                                supplier = aligned_list[k][f]
                            f = f + 1  # 循环扫描列的索引
                # 将表格竖向转换为横向表格，通过字典的形式呈现
                dict_table = {}
                for row in aligned_list:
                    if len(row) % 2 == 0:
                        for i in range(0, len(row), 2):
                            if i + 1 < len(row):
                                dict_table[row[i]] = row[i + 1]
                if dict_table:
                    # 获取字典中所有的键并打印
                    keys = dict_table.keys()
                    for key in keys:
                        if '品牌' in key and brand == '':
                            brand = dict_table[key]
                        if any(th in key for th in
                               succeed_prices) and succeed_price == '' and from_data_1_cannels == 'succeed':
                            succeed_price = dict_table[key]
                        if any(th in key for th in
                               bidding_prices) and bidding_prices == '' and (
                                        from_data_1_cannels == 'bidding' or from_data_1_cannels == 'cgall'):
                            bidding_price = dict_table[key]
                        if any(th in key for th in suppliers) and supplier == '':
                            supplier = dict_table[key]
    # 扫描网页所有文本
    texts = html_3.xpath('//text()')
    for text in texts:
        text = text.replace("\n", "").replace("\r", "").strip()
        if '品牌' in text and brand == '' and re.search(r"[:：]\s*(.*)(?:，|$)", text) != None:
            brand = re.search(r"[:：]\s*(.*)(?:，|$)", text).group(1)
        for th in suppliers:
            pattern = r"{th}：(.*?)(?:，|$)".format(th=th)
            if th in text and supplier == '' and re.search(pattern, text) != None:
                supplier = re.search(pattern, text).group(1)
        if Information_category == 'succeed':
            for th in succeed_prices:
                pattern = r"{th}[^：]*：(.*?)(?:，|$)".format(th=th)
                if th in text and succeed_price == '' and re.search(pattern, text) != None:
                    succeed_price = re.search(pattern, text).group(1)
        if Information_category == 'bidding':
            for th in bidding_prices:
                pattern = r"{th}[^：]*：(.*?)(?:，|$)".format(th=th)
                if th in text and bidding_price == '' and re.search(pattern, text) != None:
                    bidding_price = re.search(pattern, text).group(1)

    # 下载网页中存在的附件
    Appendix = ''
    if html_3.xpath('//a[@target="_parent"]'):
        for appendix in html_3.xpath('//a[@target="_parent"]'):
            appendix_URL = appendix.xpath('.//@href')[0]
            appendix_name = appendix.xpath('.//text()')[0]
            Appendix = '\n' + Appendix + appendix_name + ' :' + appendix_URL
    return brand, supplier, succeed_price, bidding_price, Appendix
```
**简介：**
这段代码用于从给定的HTML内容（html_3）中分析表格信息和文本信息，并提取品牌（brand）、供应商（supplier）、中标金额（succeed_price）、招标金额（bidding_price）以及网页中存在的附件信息。

**详细步骤：** 
1.	定义了一些变量（brand, supplier, succeed_price, bidding_price，Appendix），用于存储提取的数据。这些变量的初始值为空字符串，将用于存储表格中的特定字段以及其他信息。
2.	使用XPath从HTML内容中提取所有的<table>元素，这些<table>元素表示网页中的表格。
3.	遍历每个<table>元素，根据表格的结构和内容将其构造为一个二维列表 table。
4.	使用正则表达式和关键词匹配来提取品牌、供应商、中标金额和招标金额等字段信息。
5.	使用XPath从HTML内容中提取所有的文本，并遍历文本来进一步提取品牌、供应商、中标金额和招标金额等字段信息。
6.	如果网页中存在附件（a元素），则提取附件的名称和URL信息。
7.	返回提取到的品牌、供应商、中标金额、招标金额和附件信息。

### 2.2.6 获取需要爬取的总页数
```python
def get_total_page_1(main_url, keyword, Information_category, start_time, end_time, cookie):
    headers_1 = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Cache-Control': 'max-age=0',
        'Connection': 'keep-alive',
        'Content-Length': '233',
        'Content-Type': 'application/x-www-form-urlencoded',
        'Cookie': cookie,
        'Host': 's.zhaobiao.cn',
        'Origin': 'https://s.zhaobiao.cn',
        'Referer': 'https://s.zhaobiao.cn/s',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.170 Safari/537.36'
    }
    from_data_1 = {
        'queryword': keyword,
        'channels': Information_category,
        'starttime': start_time,
        'endtime': end_time,
        'currentpage': 1
    }
    response_1 = requests.post(main_url, headers=headers_1, data=from_data_1)
    html_1 = etree.HTML(response_1.text)
    total_page = html_1.xpath('//form/input[@id ="totalPage"]/@value')[0]
    return total_page
```
**示例：**
爬取一年内的数据（与访问历史数据库数据类似）

**简介：**
这段代码用于获取给定主URL（main_url）上搜索结果的总页数。

**详细步骤：**
1.	定义了HTTP请求头（headers_1），其中包含了一些常见的请求头字段，如User-Agent、Cookie等。这些请求头信息用于模拟一个普通的浏览器请求，以便能够访问网站并获取数据。
2.	定义了表单数据（from_data_1），包含了搜索请求的参数，如关键字、信息类别、起始时间、结束时间和当前页数。这里设置当前页数为1，因为我们只是用来获取总页数。
3.	使用requests.post方法发送POST请求到main_url，并传递请求头和表单数据。这个请求将触发对main_url的搜索操作，并返回一个包含搜索结果的网页响应。
4.	使用etree.HTML方法解析网页响应的文本内容，将其转换成一个XPath可解析的格式。
5.	使用XPath表达式//form/input[@id ="totalPage"]/@value从解析后的HTML中提取总页数（total_page）的值。这个XPath表达式可能是用于从搜索结果页面中提取总页数的输入框元素的value属性。
6.	返回提取到的总页数。

# 3 数据获取设计思路
## 3.1 设计原因
由于网页的内容被编码，在通过urllib.parse.unquote解码后，网页表格中数据呈现如图：
![解码后的数据]({{site.baseurl}}/images/zhaobiao_spider/7.jpg)
而我们要获取的是中标供应商名称，品牌，中标金额等数据，在解码的数据中并没有专门的标签对应这些数据，想获取的数据也不是键值对关系（如：中标供应商名称：理邦），而且在不同页面有着不同标签，所以这是获取特定数据的困难所在。

## 3.2 设计思路
### 3.2.1 表格形式
根据网页表格的特性，如果其是标准的列索引表格，则可以通过构建二维列表的形式来还原表格。

**标准表格示例：**
![]({{site.baseurl}}/images/zhaobiao_spider/5.jpg)
**还原的二维列表：**
[['序号', '标项名称', '规格型号', '数量', '单位', '总价(元)', '中标供应商名称', '中标供应商地址', '中标供应商统一社会信用代码'], ['1', '墨玉县乌尔其乡卫生院医疗能力发展项目', '采购一批医用设备(具体详见竞争性磋商文件)；', '1', '批', '报价:898000(元)', '上饶民辉贸易有限公司', '江西省上饶经济技术开发区拓展路力能产业园2栋A3147', '91361100MA3AD1BEX4']]

### 3.2.2 非表格形式
像网页中的非表格形式（及文本形式），如果文本以标准键值对形式出现，则可以获取到数据。

**标准键值对形式：**
![]({{site.baseurl}}/images/zhaobiao_spider/6.jpg)
通过获取网页的text信息并通过re正则表达式则可以匹配到相关数据。

# 4 Streamlit Cloud部署
## 4.1 streamlit库
Streamlit是一个用于快速构建数据应用程序和Web界面的Python库。它的设计目标是让数据科学家、工程师和研究人员能够以简单、快速的方式将数据和模型可视化，并将其部署为交互式Web应用程序，而无需深入学习前端开发或设计。
## 4.2 Streamlit Cloud
Streamlit Cloud是Streamlit官方提供的托管服务，用于轻松部署和分享基于Streamlit构建的数据应用程序和Web界面。它使用户能够将Streamlit应用程序部署到云端服务器上，并与其他人共享和访问这些应用程序。
1.	轻松部署: Streamlit Cloud提供了简单、快速的部署过程。用户可以直接将本地开发的Streamlit应用程序上传到Streamlit Cloud，而不需要复杂的服务器配置或容器化过程。
2.	实时更新: 一旦部署到Streamlit Cloud上，应用程序将自动与数据源同步，并实时更新应用程序的内容和结果。这意味着用户可以实时查看数据的最新状态，无需手动刷新页面。
3.	无服务器架构:  Streamlit Cloud采用无服务器架构，意味着用户无需管理或维护服务器。Streamlit Cloud会自动处理服务器资源的分配和管理，让用户专注于应用程序的开发和更新。

## 4.3 通过github部署到streamlit cloud
将代码存储在 GitHub 仓库中，并通过 Streamlit Cloud 自动部署和更新应用程序。以下是通过 GitHub 部署到 Streamlit Cloud 的步骤：

1.	创建 Streamlit 应用程序: 首先，使用 Streamlit 构建您的数据应用程序。确保它在本地运行正常，并将所有必要的文件和依赖项包含在您的代码库中。在本地运行应用程序时，在终端输入：streamlit run app.py（app.py是你的应用程序）。
2.	将应用程序上传到 GitHub: 将您的应用程序代码上传到 GitHub 仓库。确保您的代码库包含了所有必要的文件，例如主应用程序文件 app.py、依赖项文件 requirements.txt 或 environment.yml、以及其他需要的资源文件。
3.	在 Streamlit Cloud 创建应用: 登录 Streamlit Cloud 账号后，在 Streamlit Cloud 仪表板中点击 "New App" 创建一个新的应用。在创建过程中，您将被要求选择从 GitHub 部署应用程序。
4.	连接 GitHub 仓库: 在创建应用时，选择 "Connect to GitHub" 选项，并允许 Streamlit Cloud 访问您的 GitHub 仓库。通过这种方式，Streamlit Cloud 将能够获取您的代码并部署应用程序。
5.	选择应用程序源代码: 在连接到 GitHub 后，选择您要部署的应用程序的源代码仓库和分支。
6.	配置部署设置: 在连接到 GitHub 之后，您可以配置部署设置，例如应用程序的名称、图标、描述等。
7.	点击部署: 确认所有设置后，点击 "Deploy" 按钮。Streamlit Cloud 将自动从 GitHub 仓库获取代码，并在云端部署应用程序。
8.	等待部署完成: 部署完成后，Streamlit Cloud 将提供一个链接，您可以通过浏览器访问您的应用程序。
9.	实时更新: 一旦部署到 Streamlit Cloud，您的应用程序将与 GitHub 仓库实时同步，并自动更新应用程序的内容和结果。

**细节展示：**
1.	在github中创建仓库，并将主应用程序文件 app.py、依赖项文件 requirements.txt和packages.txt上传到仓库。
![]({{site.baseurl}}/images/zhaobiao_spider/2.jpg)
2.	登录streamlit cloud，点击new app，选择相应的github库。
![]({{site.baseurl}}/images/zhaobiao_spider/3.jpg)
3.	点击部署，等待streamlit cloud部署完成后即可使用，以后便通过github来管理和更新app。
![]({{site.baseurl}}/images/zhaobiao_spider/4.jpg)

# 附录
```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from PIL import Image
import ddddocr
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
import time
import streamlit as st
import base64
import requests
from lxml import etree
import re
import urllib.parse
from io import BytesIO
import pandas as pd
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
def Get_Cookies(url_login, url_target, user_name, secret):
    strr = ''  # 创建空的cookie值
    with st.spinner('Loading cookie...'):
        while (True):
            options = Options()
            options.add_argument('--disable-gpu')
            options.add_argument('--headless')
            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
            driver.get(url_login)

            zhanghao_mima = driver.find_element(By.ID, 'zh')
            zhanghao_mima.click()
            # 定位账号，密码，验证码和登录按钮部分
            username_input = driver.find_element(By.ID, 'loginUserId')
            password_input = driver.find_element(By.ID, 'loginPassword')
            captcha = driver.find_element(By.ID, 'yzm')
            login_button = driver.find_element(By.CLASS_NAME, 'login_button')
            username_input.send_keys(user_name)  # 填写用户名 zhjk2019
            password_input.send_keys(secret)  # 填写密码 535cmeyr
            # 识别验证码部分
            png = driver.find_element(By.ID, 'randimg')
            screenshot = png.screenshot_as_png  # 获取屏幕截图的二进制数据
            image_stream = BytesIO(screenshot)  # 使用BytesIO创建一个内存文件对象
            image = Image.open(image_stream)  # 通过内存文件对象加载图像
            ocr = ddddocr.DdddOcr()  # 验证码识别库
            res = ocr.classification(image)
            captcha.send_keys(res)  # 输入识别的验证码
            login_button.click()
            # 等待页面跳转
            wait = WebDriverWait(driver, 10)
            try:
                wait.until(EC.url_to_be(url_target))
                time.sleep(5)
                cookie = driver.get_cookies()
                strr = ''
                for c in cookie:
                    strr += c['name']
                    strr += '='
                    strr += c['value']
                    strr += ';'
                st.success('获取cookies成功！')
                image_stream.close()  # 关闭内存文件对象
                break
            except:
                image_stream.close()  # 关闭内存文件对象
                continue
    return strr

def Create_dataframe():
    # 为创建的excel表格添加表头
    data_list = ['类型', '标题', '地区', '发布时间', '品牌', '供应商', '中标金额', '招标金额', '标的物', '对应网址',
                 '附件']
    # 创建空的DataFrame
    df = pd.DataFrame(columns=data_list)
    return df

def Analyze_Main_Url_1(main_url, keyword, Information_category, start_time, end_time, i, cookie):
    headers_1 = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Cache-Control': 'max-age=0',
        'Connection': 'keep-alive',
        'Content-Length': '233',
        'Content-Type': 'application/x-www-form-urlencoded',
        'Cookie': cookie,
        'Host': 's.zhaobiao.cn',
        'Origin': 'https://s.zhaobiao.cn',
        'Referer': 'https://s.zhaobiao.cn/s',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.170 Safari/537.36'
    }
    from_data_1 = {
        'queryword': keyword,
        'channels': Information_category,
        'starttime': start_time,
        'endtime': end_time,
        'currentpage': i
    }
    response_1 = requests.post(main_url, headers=headers_1, data=from_data_1)
    html_1 = etree.HTML(response_1.text)
    detail_Urls = html_1.xpath('.//tr[@class = "datatr"]')
    return detail_Urls

def Analyze_Main_Url_2(main_url, keyword, Information_category, year, month, i, cookie):
    headers_1 = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Cache-Control': 'max-age=0',
        'Connection': 'keep-alive',
        'Content-Length': '233',
        'Content-Type': 'application/x-www-form-urlencoded',
        'Cookie': cookie,
        'Host': 's.zhaobiao.cn',
        'Origin': 'https://s.zhaobiao.cn',
        'Referer': 'https://s.zhaobiao.cn/s',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.170 Safari/537.36'
    }
    from_data_1 = {
        'hisType': 'zb',
        'queryword': keyword,
        'channels': Information_category,
        'year': year,
        'month': month,
        'currentpage': i
    }
    response_2 = requests.post(main_url, headers=headers_1, data=from_data_1)
    html_2 = etree.HTML(response_2.text)
    detail_Urls = html_2.xpath('.//tr[@style = "border-bottom:1px dashed #ccc;"]')
    return detail_Urls

def Sift_Url(detail_Url, index, keyword_list, cookie):
    headers_2 = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Cache-Control': 'max-age=0',
        'Connection': 'keep-alive',
        'Content-Length': '46',
        'Content-Type': 'application/x-www-form-urlencoded',
        'Cookie': cookie,
        'Host': 'zb.zhaobiao.cn',
        'Origin': 'https://s.zhaobiao.cn',
        'Referer': 'https://s.zhaobiao.cn/s',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.170 Safari/537.36'
    }
    from_data_2 = {
        'q': '%D0%C4%B5%E7%CD%F8%C2%E7',
        'm': '1'
    }
    url = detail_Url.xpath('td/a/@href')[index]
    st.write("正在爬取的网页是:", url)
    response_2 = session.post(url, headers=headers_2, data=from_data_2)
    html_2 = etree.HTML(response_2.text)
    contents = html_2.xpath('.//div[@class = "w-noticeCont"]/div[@class = "w-contIn page"]/script/text()')[0].strip()
    # 网页信息被编码了，提取编码
    secret = re.findall('var ss = "(.*)";', contents)[0]
    # 解码
    pass_word = urllib.parse.unquote(urllib.parse.unquote(secret))
    html_3 = etree.HTML(pass_word)
    texts = html_3.xpath('//text()')
    combine_texts = ' '.join(texts)
    for th in keyword_list:
        if th in combine_texts:
            type = detail_Url.xpath('td[@align="center"]/text()')[0]  # 类型
            title = html_2.xpath('//h1[@id="infotitle"]/text()')[0]  # 标题
            place = detail_Url.xpath('td[@align="center"]/text()')[1]  # 地点
            release_time = detail_Url.xpath('td[@align="center"]/text()')[2]  # 时间
            return type, title, place, release_time, html_3, url, th

def analysis_detail_Url(from_data_1_cannels, html_3, thing):
    brand = ''
    supplier = ''
    succeed_price = ''
    bidding_price = ''
    # 构建网页中的表格信息
    table_elements = html_3.xpath('//table')
    for table_element in table_elements:  # 遍历网页中所有的表格（构造表格为竖状表格）
        table = []
        # 确认解码网页中是否含有thead标签
        thead_elements = table_element.xpath('.//thead/th')
        if thead_elements:
            for thead in thead_elements:
                table.append(thead.xpath('.//th/text()'))
        for trs in table_element.xpath('.//tr'):
            table_2 = []
            # 确认解码网页中是否含有th标签
            if len(trs.xpath('.//th')) > 1:
                table.append(trs.xpath('.//th/text()'))
            else:
                for td in trs.xpath('.//td'):
                    if td.xpath('.//text()'):
                        if td.xpath('.//p'):
                            table_2.append(td.xpath('.//p')[0].xpath('string()').replace('\n', '').replace(' ', ''))
                        elif td.xpath('.//div'):
                            table_2.append(td.xpath('.//div')[0].xpath('string()').replace('\n', '').replace(' ', ''))
                        else:
                            table_2.append(td.xpath('string()').replace('\n', '').replace(' ', ''))
                    else:
                        table_2.append('-')
            # 如果table_2不空，添加到table表格
            if len(table_2) > 0:
                table.append(table_2)
        if len(table) != 0:
            # 获取每行的最大列数
            max_cols = max(len(row) for row in table)
            # 输出对齐后的二维列表
            aligned_list = [row + ['-'] * (max_cols - len(row)) for row in table]
            f = 0  # 扫描列的初始索引
            if len(aligned_list) > 1:
                for index, row in enumerate(aligned_list):  # 输出关键词所在的行索引
                    row = ' '.join(row)
                    if thing in row or len(aligned_list) <= 2:
                        if thing in row:
                            k = index
                        else:
                            k = 1
                        for columns in aligned_list[0]:
                            if '品牌' in columns and brand == '':
                                brand = aligned_list[k][f]
                            if any(th in columns for th in
                                       succeed_prices) and succeed_price == '' and from_data_1_cannels == 'succeed':
                                succeed_price = aligned_list[k][f]
                            if any(th in columns for th in bidding_prices) and bidding_price == '' and (
                                        from_data_1_cannels == 'bidding' or from_data_1_cannels == 'cgall'):
                                bidding_price = aligned_list[k][f]
                            if any(th in columns for th in suppliers) and supplier == '':
                                supplier = aligned_list[k][f]
                            f = f + 1  # 循环扫描列的索引
                # 将表格竖向转换为横向表格，通过字典的形式呈现
                dict_table = {}
                for row in aligned_list:
                    if len(row) % 2 == 0:
                        for i in range(0, len(row), 2):
                            if i + 1 < len(row):
                                dict_table[row[i]] = row[i + 1]
                if dict_table:
                    # 获取字典中所有的键并打印
                    keys = dict_table.keys()
                    for key in keys:
                        if '品牌' in key and brand == '':
                            brand = dict_table[key]
                        if any(th in key for th in
                               succeed_prices) and succeed_price == '' and from_data_1_cannels == 'succeed':
                            succeed_price = dict_table[key]
                        if any(th in key for th in
                               bidding_prices) and bidding_prices == '' and (
                                        from_data_1_cannels == 'bidding' or from_data_1_cannels == 'cgall'):
                            bidding_price = dict_table[key]
                        if any(th in key for th in suppliers) and supplier == '':
                            supplier = dict_table[key]

    # 扫描网页所有文本
    texts = html_3.xpath('//text()')
    for text in texts:
        text = text.replace("\n", "").replace("\r", "").strip()
        if '品牌' in text and brand == '' and re.search(r"[:：]\s*(.*)(?:，|$)", text) != None:
            brand = re.search(r"[:：]\s*(.*)(?:，|$)", text).group(1)

        for th in suppliers:
            pattern = r"{th}：(.*?)(?:，|$)".format(th=th)
            if th in text and supplier == '' and re.search(pattern, text) != None:
                supplier = re.search(pattern, text).group(1)

        if Information_category == 'succeed':
            for th in succeed_prices:
                pattern = r"{th}[^：]*：(.*?)(?:，|$)".format(th=th)
                if th in text and succeed_price == '' and re.search(pattern, text) != None:
                    succeed_price = re.search(pattern, text).group(1)

        if Information_category == 'bidding':
            for th in bidding_prices:
                pattern = r"{th}[^：]*：(.*?)(?:，|$)".format(th=th)
                if th in text and bidding_price == '' and re.search(pattern, text) != None:
                    bidding_price = re.search(pattern, text).group(1)

    # 下载网页中存在的附件
    Appendix = ''
    if html_3.xpath('//a[@target="_parent"]'):
        for appendix in html_3.xpath('//a[@target="_parent"]'):
            appendix_URL = appendix.xpath('.//@href')[0]
            appendix_name = appendix.xpath('.//text()')[0]
            Appendix = '\n' + Appendix + appendix_name + ' :' + appendix_URL

    return brand, supplier, succeed_price, bidding_price, Appendix

def get_total_page_1(main_url, keyword, Information_category, start_time, end_time, cookie):
    headers_1 = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Cache-Control': 'max-age=0',
        'Connection': 'keep-alive',
        'Content-Length': '233',
        'Content-Type': 'application/x-www-form-urlencoded',
        'Cookie': cookie,
        'Host': 's.zhaobiao.cn',
        'Origin': 'https://s.zhaobiao.cn',
        'Referer': 'https://s.zhaobiao.cn/s',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.170 Safari/537.36'
    }
    from_data_1 = {
        'queryword': keyword,
        'channels': Information_category,
        'starttime': start_time,
        'endtime': end_time,
        'currentpage': 1
    }
    response_1 = requests.post(main_url, headers=headers_1, data=from_data_1)
    html_1 = etree.HTML(response_1.text)
    total_page = html_1.xpath('//form/input[@id ="totalPage"]/@value')[0]
    return total_page

def get_total_page_2(main_url, keyword, Information_category, year, month, cookie):
    headers_1 = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Accept-Language': 'zh-CN,zh;q=0.9',
        'Cache-Control': 'max-age=0',
        'Connection': 'keep-alive',
        'Content-Length': '233',
        'Content-Type': 'application/x-www-form-urlencoded',
        'Cookie': cookie,
        'Host': 's.zhaobiao.cn',
        'Origin': 'https://s.zhaobiao.cn',
        'Referer': 'https://s.zhaobiao.cn/s',
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.170 Safari/537.36'
    }
    from_data_1 = {
        'hisType': 'zb',
        'queryword': keyword,
        'channels': Information_category,
        'year': year,
        'month': month,
        'currentpage': 1
    }
    response_2 = requests.post(main_url, headers=headers_1, data=from_data_1)
    html_2 = etree.HTML(response_2.text)
    total_page = html_2.xpath('//form/input[@id ="totalPage"]/@value')[0]
    return total_page

def main(total_page):
    for i in range(1, int(total_page) + 1):
        if choose == '爬取一年内的数据':
            detail_urls = Analyze_Main_Url_1(main_url, keyword, Information_category, start_time, end_time, i, cookie)
        else:
            detail_urls = Analyze_Main_Url_2(main_url, keyword, Information_category, year, month, i, cookie)
        st.write(f'========================共需爬取{total_page}页，正在爬取第{i}页========================')
        for detail_url in detail_urls:
            LIST = []
            j = 0
            try:
                if Sift_Url(detail_url, j, keyword_list, cookie):
                    type, title, place, release_time, html_3, url, thing = Sift_Url(detail_url, j, keyword_list, cookie)
                    brand, supplier, succeed_price, bidding_price, appendix = analysis_detail_Url(Information_category, html_3, thing)
                    st.write("品牌：", brand)
                    st.write("供应商：", supplier)
                    if Information_category == 'succeed':
                        st.write("中标金额：", succeed_price)
                    if Information_category == 'bidding':
                        st.write("招标金额：", bidding_price)
                    st.write("标的物：", thing)
                    st.write("附件：", appendix)
                    # 合成二维列表
                    LIST.append(type)
                    LIST.append(title)
                    LIST.append(place)
                    LIST.append(release_time)
                    LIST.append(brand.replace(' ', ''))
                    LIST.append(supplier.replace(' ', ''))
                    LIST.append(succeed_price.replace(' ', ''))
                    LIST.append(bidding_price.replace(' ', ''))
                    LIST.append(thing.replace("/", ""))
                    LIST.append(url)
                    LIST.append(appendix)
                    df.loc[len(df)] = LIST
                    j = j + 1
            except:
                j += 1

def download_df_to_excel():
    # 创建一个内存文件对象
    excel_buffer = BytesIO()
    # 将 DataFrame 保存到内存文件对象中
    with pd.ExcelWriter(excel_buffer, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False, sheet_name='Sheet1')
    # 获取内存文件对象的二进制数据
    excel_data = excel_buffer.getvalue()
    # 提供下载链接
    b64 = base64.b64encode(excel_data).decode()
    href = f'<a href="data:application/octet-stream;base64,{b64}" download="data.xlsx">Download Excel File</a>'
    st.markdown(href, unsafe_allow_html=True)

if __name__ == '__main__':
    # 获取cookies的urls
    login_url = 'http://user.zhaobiao.cn/login.html'
    url_target = 'https://user.zhaobiao.cn/homePageUc.do'
    # 查询网页所需要的关键字
    suppliers = ['供应商名称', '成交企业', '拟成交公司', '成交单位', '中标人', '成交候选人', '制造商', '成交供应商',
                 '中选机构', '拟中标公司', '中选人', '中标 (成交)单位']
    succeed_prices = ['成交价（元）', '中标金额', '中标（成交）金额', '总价(元)', '成交金额（元）',
                      '中标（成交）价', '成交价格', '拟成交金额', '中标（成交）结果', '报价（元）', '中选金额']
    bidding_prices = ['预算金额', '控制金额(元)', '招标控制价', '品目预算(元)', '最高限价', '项目预算',
                      '成交价（元）', '采购预算金额 （元）', '采购预算']
    # 创建一个会话对象保持动态连接
    session = requests.Session()
    st.set_page_config(page_title="招标网爬取工具")
    st.title("招标网爬取工具")   # Streamlit应用程序的标题和描述
    user_name = st.sidebar.text_input('输入用户名')
    secret = st.sidebar.text_input('输入密码')
    choose = st.sidebar.selectbox('爬取数据选项', ['爬取一年内的数据', '访问历史库往年数据'])
    if choose == '爬取一年内的数据':
        year_data = st.container()
        with st.form('year_data'):
            # 爬取的主页面
            main_url = 'https://s.zhaobiao.cn/s'
            df = Create_dataframe()
            keyword = st.text_input("请输入产品的关键词：")
            keyword_list = st.text_input("请输入具体的关键词（多个关键字用空格分隔）：").split()
            Information_category = st.text_input(
                '请根据提示输入你需要爬取的信息类别（如：招标公告、中标公告、采购公告）：')
            if Information_category == '招标公告':
                Information_category = 'bidding'
            elif Information_category == '中标公告':
                Information_category = 'succeed'
            elif Information_category == '采购公告':
                Information_category = 'cgall'
            else:
                st.warning('输入错误')
            start_time = st.text_input('请输入你要爬取的开始时间（如：20220605):')
            end_time = st.text_input('请输入你要爬取的结束时间（如：20220605):')
            submit_button = st.form_submit_button('开始爬取')
        if submit_button:
            cookie = Get_Cookies(login_url, url_target, user_name, secret)
            total_page_1 = get_total_page_1(main_url, keyword, Information_category, start_time, end_time, cookie)
            time.sleep(5)
            main(total_page_1)
            download_df_to_excel()
            st.write(df)
    if choose == '访问历史库往年数据':
        history_data = st.container()
        with st.form('history_data'):
            main_url = 'https://s.zhaobiao.cn/search/history/result'
            df = Create_dataframe()
            keyword = st.text_input("请输入产品的关键词：")
            keyword_list = st.text_input("请输入具体的关键词（多个关键字用空格分隔）：").split()
            Information_category = st.text_input(
                '请根据提示输入你需要爬取的信息类别（如：招标公告、中标公告、采购公告）：')
            if Information_category == '招标公告':
                Information_category = 'bidding'
            elif Information_category == '中标公告':
                Information_category = 'succeed'
            elif Information_category == '采购公告':
                Information_category = 'cgall'
            year = st.text_input('请输入你要需要访问的年份（如：2022):')
            month = st.text_input('请输入你要访问的月份（如：01):')
            submit_button = st.form_submit_button('开始爬取')
        if submit_button:
            cookie = Get_Cookies(login_url, url_target, user_name, secret)
            total_page_2 = get_total_page_2(main_url, keyword, Information_category, year, month, cookie)
            time.sleep(5)
            main(total_page_2)
            download_df_to_excel()
            st.write(df)
```
